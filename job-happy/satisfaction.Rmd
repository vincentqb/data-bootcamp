---
author: "Vincent Quenneville-BÃ©lair"
date: "February, 2016"
output: pdf_document
---

# Loading data

Our goal is to predict job satisfaction using the survey.
```{r}
library(foreign)

library(dplyr)
library(caret)
library(pROC)

library(e1071)
library(rpart)

library(ggplot2)
library(rattle)
```

Load SPSS file as data frame.
```{r}
filename <- "Materials for Public Access on Web/PST July-06 finalc.sav"
dataset.original <- read.spss(filename, to.data.frame = TRUE)
dataset <- dataset.original

# Take a quick look at dataset
# names(dataset)
# summary(dataset)
# head(dataset)
# length(dataset)
# str(dataset)
```

Job satisfaction is found in q42. We turn this problem into a binary classification problem: either the respondent explicitly declared satisfaction ("S") or not ("N").
```{r}
# Classify the respondent as happy if they clearly indicated so in the survey
dataset$q42 <- ifelse(dataset$q42 == "Completely satisfied" | dataset$q42 == "Mostly satisfied", "S", "N")
# dataset$q42[is.na(dataset$q42)] <- "N"
dataset$q42 <- as.factor(dataset$q42)
```

# Selecting columns

We select only a few columns that we think are of interest. Having to many features may result in overfitting.
```{r}
keep <- c("q42", "q43", "q1", "q2", "q3", "q4", "q26", "q39", "q40", "q41", "q44")
# keep <- c("q42", "q43", "q1", "q2", "q3", "q4", "q26", "q39", "q40", "q41", "q44", "q45a", "q45b", "q45c", "q45d", "q45e", "q45f", "q45g", "q45h", "q45i", "q45j")

dataset.keep <- dataset[,keep]
dataset <- dataset.keep

dim(dataset)
```

Instead, we could drop a few columns that do not appear to contain a significant amount of information.
```{r}
# # Drop columns that appear to have low variability
# drops1 <- c("tz","dow","Q18.2", "citizen", "usborn1b", "usb1bos", "religos", "race","hisp", "raceos", "partln", "pvote04a", "labor", "q6bvb", "q6wvb", "q17vs", "q17vb", "q18vb", "q19os", "q30os", "q32os")
# # Drop technical columns for survey
# drops2 <- c("wt_gp", "totwt", "sample", "net1", "net2", "version", "form")
# # Drop unknown columns
# drops3 <- c("density", "born", "psraid", "int_date", "area", "msa", "fips")
# # After reading questionaire, those are not expected to be correlated
# drops4 <- c("q7f1", "q8", "q9", "q10", "q11f2", "q12", "q13", "q14", "q15", "q16", "q17", "q18", "q19", "q22", "q23", "net1", "net2", "website")
# 
# drops <- c(drops1, drops2, drops3, drops4)
# # dataset.drop <- dataset %>% select(-drops)
# dataset.drop <- dataset[,!(names(dataset) %in% drops)]
# dataset <- dataset.drop
```

We drop near zero variance columns.
```{r}
nzv <- nearZeroVar(dataset, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
nzv[nzv$nzv,]

if (sum(nzv$nzv) > 0) {
  dataset.nzv <- dataset[,-which(nzv$nzv)]
  dataset.nzv <- dataset[,-nzv$nzv]
  dataset <- dataset.nzv
}

dim(dataset)
```

We drop the columns with too many NAs, except columns to predict.
```{r}
thresold.na <- 1000
dataset.thresold <- dataset[,colSums(is.na(dataset)) < thresold.na]
# dataset.thresold <- cbind(dataset.thresold, select(dataset, q42))
dataset <- dataset.thresold

dim(dataset)
```

We divide the dataset into train and test.
```{r}
set.seed(1)

trainIndex <- createDataPartition(dataset$q42, p = .8, list = FALSE)
training <- dataset[ trainIndex,]
testing  <- dataset[-trainIndex,]
```

We split the data frame into numerics and factors.
```{r}
pos <- which(sapply(dataset, is.numeric))

training.fac <- select(training, -pos)
training.num <- select(training,  pos)
testing.fac <- select(testing, -pos)
testing.num <- select(testing,  pos)
```

We now fill the missing numerical values.
```{r}
preProcValues <- preProcess(training.num, method = c("center", "scale", "knnImpute", "BoxCox", "YeoJohnson"))
training.num <- predict(preProcValues, training.num)
testing.num <- predict(preProcValues, testing.num)
```

We find correlations between numerical columns.
```{r}
descrCor <- cor(training.num)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)

# Remove correlated columns if found any
if (length(highlyCorDescr) > 0) {
  training.num <- training.num[, -highlyCorDescr];
  testing.num <- testing.num[, -highlyCorDescr]
}
```

We now remove numerical columns that are linearly dependent.
```{r}
comboInfo <- findLinearCombos(training.num)
comboInfo

if (length(comboInfo$remove) > 0) {
  training.num <- training.num[, -comboInfo$remove];
  testing.num <- testing.num[, -comboInfo$remove]
}
```

We drop the rows that do not have complete information.
```{r}
rows <- complete.cases(training)
training.complete <- training[rows,]
training <- training.complete

dim(training)
```

We create dummy variables for categorical variables.
```{r}
# dummies <- dummyVars(~ ., data = training, sep = "__", drop2nd = TRUE)
# training <- data.frame(predict(dummies, training))
# testing <- data.frame(predict(dummies, testing))
```

We verify how many observations per predictors we have.
```{r}
nrow(training)/ncol(training)
```

# Predicting

We are ready to train a model.
```{r}
# Cross-Validation
ctrl <- trainControl(method = "cv", number = 10, verboseIter = T)

# Select method
# method <- "rf"
# method <- "rpart"
method <- "glmnet"
# method <- "svmLinear"
# method <- "svmRadial"

if (method == "rf") {
  modelGrid <- expand.grid(mtry = seq(0, 5, by = 1))
} else if (method == "rpart") {
  modelGrid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
} else if (method == "glmnet") {
  # alpha between 0 (ridge) and 1 (lasso)
  modelGrid <- expand.grid(.alpha = c(0, 0.1, 0.5, 0.7, 1), .lambda = seq(0, 10, by = 0.1)) 
} else if (method == "svmLinear") {
  modelGrid <- expand.grid(C = seq(0.5, 5, by = 0.5))
} else if (method == "svmRadial") {
  modelGrid <- expand.grid(C = seq(0.5, 5, by = 0.5), sigma = seq(0.1, 0.5, by = 0.1))
}

# We can try different methods: svmLinear glmnet rpart svmRadial
modelTune <- train(q42 ~ ., 
                   data = training, 
                   tuneGrid = modelGrid, 
                   trControl = ctrl, 
                   method = method, 
                   # summaryFunction = twoClassSummary,
                   # metric = "ROC", 
                   # classProbs = TRUE, 
                   # importance = TRUE, 
                   # scale = FALSE,
                   na.action = na.pass)

modelTune
varImp(modelTune)
plot(modelTune)

modelTune$bestTune
plot(modelTune$results)

par(mfrow = c(1,1))
plot(modelTune$finalModel)

# fancyRpartPlot(modelTune$finalModel)
```
The most important columns seem to be q2, q43, q3 and q4, depending on the model.

We evaluate using the ROC curve.
```{r}
probsTrain <- predict(modelTune, 
                      training, 
                      type = "prob", 
                      na.action = na.pass)

rocCurve   <- roc(response = training$q42,
                      predictor = probsTrain$S,
                      levels = rev(levels(training$q42)))

plot(rocCurve, print.thres = "best")

names(rocCurve)
rocCurve$thresholds
rocCurve$sensitivity

# choose threshold
qplot(x = rocCurve$thresholds, y = rocCurve$sensit)
qplot(x = rocCurve$thresholds, y = rocCurve$spec)
```

We now predict on the test set.
```{r}
modelPredict <- predict(modelTune, 
                        newdata = testing, 
                        # type = "prob", 
                        na.action = na.pass)

# thresold <- 0.7
# modelPredict <- modelPredict %>%
#   mutate(class = ifelse(S > thresold, "S", "N")) %>%
#   mutate(class = as.factor(class))

par(mfrow = c(1,1))
plot(modelPredict)

table(predict = modelPredict, true = testing$q42)

rtCM <- confusionMatrix(modelPredict, testing$q42, positive = "S")
rtCM
```