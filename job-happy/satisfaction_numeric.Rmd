---
author: "Vincent Quenneville-BÃ©lair"
date: "February, 2016"
output: pdf_document
---

Our goal is to predict job satisfaction using the survey.
```{r}
library(foreign)

library(dplyr)
library(caret)
library(car)

library(e1071)
library(rpart)

library(ggplot2)
library(rattle)
```

Load SPSS file as data frame.
```{r}
filename <- "Materials for Public Access on Web/PST July-06 finalc.sav"
dataset.original <- read.spss(filename, to.data.frame=TRUE)
dataset <- dataset.original

# Take a quick look at dataset
# names(dataset)
# summary(dataset)
# head(dataset)
# length(dataset)
# str(dataset)
```

Job satisfaction is found in q42. 

We convert q42 and q43 to numeric.
```{r}
dataset$q42 <- as.numeric(factor(dataset$q42))
dataset$q43 <- as.numeric(factor(dataset$q43))
```

We drop a few columns that do not appear to contain a significant amount of information.
```{r}
# Drop columns that appear to have low variability
drops1 <- c("tz","dow","Q18.2", "citizen", "usborn1b", "usb1bos", "religos", "race","hisp", "raceos", "partln", "pvote04a", "labor", "q6bvb", "q6wvb", "q17vs", "q17vb", "q18vb", "q19os", "q30os", "q32os")
# Drop technical columns for survey
drops2 <- c("wt_gp", "totwt", "sample", "net1", "net2", "version", "form")
# Drop unknown columns
drops3 <- c("density", "born", "psraid", "int_date", "area", "msa", "fips")
# After reading questionaire, those are not expected to be correlated
drops4 <- c("q7f1", "q8", "q9", "q10", "q11f2", "q12", "q13", "q14", "q15", "q16", "q17", "q18", "q19", "q22", "q23", "net1", "net2", "website")

drops <- c(drops1, drops2, drops3, drops4)
# dataset.drop <- dataset %>% select(-drops)
dataset.drop <- dataset[,!(names(dataset) %in% drops)]
dataset <- dataset.drop
```

We drop near zero variance columns.
```{r}
nzv <- nearZeroVar(dataset, freqCut = 75/5, uniqueCut = 20, saveMetrics = TRUE)
nzv[nzv$nzv,]

# dataset.nzv <- dataset[,-which(nzv$nzv)]
dataset.nzv <- dataset[,-nzv$nzv]
dataset <- dataset.nzv
```

We drop the columns with too many NAs, except columns to predict.
```{r}
dim(dataset)
thresold.na <- 50
dataset.thresold <- dataset[,colSums(is.na(dataset)) < thresold.na]
dataset.thresold <- cbind(dataset.thresold, select(dataset, q42))
dataset <- dataset.thresold
dim(dataset)
```

We drop the rows that do not have complete information.
```{r}
dim(dataset)
rows <- complete.cases(dataset)
dataset.complete <- dataset[rows,]
dataset <- dataset.complete
dim(dataset)
```

We divide the dataset into train and test.
```{r}
set.seed(1)

trainIndex <- createDataPartition(dataset$q42, p = .8, list = FALSE)
training <- dataset[ trainIndex,]
testing  <- dataset[-trainIndex,]
```

We split the data frame into numerics and factors.
```{r}
pos <- which(sapply(dataset, is.numeric))

training.fac <- select(training, -pos)
training.num <- select(training,  pos)
testing.fac <- select(testing, -pos)
testing.num <- select(testing,  pos)
```

We now fill the missing numerical values.
```{r}
preProcValues <- preProcess(training.num, method = c("center", "scale", "knnImpute", "BoxCox", "YeoJohnson"))
training.num <- predict(preProcValues, training.num)
testing.num <- predict(preProcValues, testing.num)
```

We find correlations between numerical columns.
```{r}
descrCor <- cor(training.num)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)

# Remove correlated columns if found any
if (length(highlyCorDescr) > 0) {
  training.num <- training.num[, -highlyCorDescr];
  testing.num <- testing.num[, -highlyCorDescr]
}
```

We now remove numerical columns that are linearly dependent.
```{r}
comboInfo <- findLinearCombos(training.num)
comboInfo

if (length(comboInfo$remove) > 0) {
  training.num <- training.num[, -comboInfo$remove];
  testing.num <- testing.num[, -comboInfo$remove]
}
```

We combine the numeric and factor columns back together.
```{r}
training <- cbind(training.num, training.fac)
testing <- cbind(testing.num, testing.fac)
```

We remove the rows that have too many NAs.
```{r}
# thresold.na <- 50
# training <- training[rowSums(is.na(training)) < thresold.na,]
```

We create dummy variables for categorical variables.
```{r}
dummies <- dummyVars(~ ., data = training, sep = "__")
training <- data.frame( predict(dummies, training))
testing <- data.frame( predict(dummies, testing) )
# str(training.full)
```

We verify how many observations per predictors we have.
```{r}
nrow(training)/ncol(training)
```

We do a linear regression.
```{r}
lm.fit <- lm(q42 ~ ., data = training)

summary(lm.fit)
confint(lm.fit)

# Diagnostic plots
par(mfrow = c(2,2))
plot(lm.fit)

# Are residuals normaly distributed?
# if p-value < 0.05, residuals are NOT normaly distributed
shapiro.test(lm.fit$residuals)

# Do we have autocorrelation of residuals?
# if p-value < 0.05, residuals have autocorrelation
durbinWatsonTest(lm.fit)

# Predict
lm.predict <- predict(lm.fit, newdata = testing, interval = "confidence")
RMSE(lm.predict, testing$q42)
```

The F-statistic indicate that the relation is significant -- though the R^2 says that only 50% of the variance is explained. The factors that appear to be the most significant is q43.f and age. The residual plots do not support the linear model.

# SVM

We now try a SVM classifier.
```{r}
svm.model <- svm(q42 ~ ., data = training, cost = 100, gamma = 1)
svm.predict <- predict(svm.model, testing)
table(predict = svm.predict, true = testing[,"q42"])

svm.model

rtCM <- confusionMatrix(svm.predict, testing$q42)
rtCM
```

# Linear regression model

We keep only the column with numerical values.
```{r}
training <- training.num
testing <- testing.num
```

We do a linear regression.
```{r}
lm.fit <- lm(q42 ~ ., data = training)
lm.fit

summary(lm.fit)
confint(lm.fit)

# Diagnostic plots
par(mfrow = c(2,2))
plot(lm.fit)
```

The F-statistic indicate that the relation is significant -- though the R^2 says that only 50% of the variance is explained. The factors that appear to be the most significant is q43.f and age. The residual plots do not support the linear model.