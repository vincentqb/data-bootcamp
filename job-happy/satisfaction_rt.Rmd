---
author: "Vincent Quenneville-BÃ©lair"
date: "February, 2016"
output: pdf_document
---

Our goal is to predict job satisfaction using the survey.
```{r}
library(foreign)

library(dplyr)
library(caret)

library(e1071)
library(rpart)

library(ggplot2)
library(rattle)
```

Load SPSS file as data frame.
```{r}
filename <- "Materials for Public Access on Web/PST July-06 finalc.sav"
dataset.original <- read.spss(filename, to.data.frame=TRUE)
dataset <- dataset.original

# Take a quick look at dataset
# names(dataset)
# summary(dataset)
# head(dataset)
# length(dataset)
# str(dataset)
```

Job satisfaction is found in q42. We turn this problem into a binary classification problem: either the respondent explicitly declared satisfaction ("S"), or otherwise we assume not ("F").
```{r}
# Classify the respondent as happy if they clearly indicated so in the survey
dataset$q42 <- ifelse(dataset$q42 == "Completely satisfied" | dataset$q42 == "Mostly satisfied", "S", "F")
dataset$q42[is.na(dataset$q42)] <- "F"
dataset$q42 <- as.factor(dataset$q42)
```

We drop a few columns that do not appear to contain a significant amount of information.
```{r}
# Drop columns that appear to have low variability
drops1 <- c("tz","dow","Q18.2", "citizen", "usborn1b", "usb1bos", "religos", "race","hisp", "raceos", "partln", "pvote04a", "labor", "q6bvb", "q6wvb", "q17vs", "q17vb", "q18vb", "q19os", "q30os", "q32os")
# Drop technical columns for survey
drops2 <- c("wt_gp", "totwt", "sample", "net1", "net2", "version", "form")
# Drop unknown columns
drops3 <- c("density", "born", "psraid", "int_date", "area", "msa", "fips")
# After reading questionaire, those are not expected to be correlated
drops4 <- c("q7f1", "q8", "q9", "q10", "q11f2", "q12", "q13", "q14", "q15", "q16", "q17", "q18", "q19", "q22", "q23", "net1", "net2", "website")

drops <- c(drops1, drops2, drops3, drops4)
# dataset.drop <- dataset %>% select(-drops)
dataset.drop <- dataset[,!(names(dataset) %in% drops)]
dataset <- dataset.drop
```

We drop near zero variance columns.
```{r}
nzv <- nearZeroVar(dataset, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
nzv[nzv$nzv,]

# dataset.nzv <- dataset[,-which(nzv$nzv)]
dataset.nzv <- dataset[,-nzv$nzv]
dataset <- dataset.nzv
```

We drop the columns with too many NAs, except columns to predict.
```{r}
dim(dataset)
thresold.na <- 1000
dataset.thresold <- dataset[,colSums(is.na(dataset)) < thresold.na]
# dataset.thresold <- cbind(dataset.thresold, select(dataset, q42))
dataset <- dataset.thresold
dim(dataset)
```

We drop the rows that do not have complete information.
```{r}
# dim(dataset)
# rows <- complete.cases(dataset)
# dataset.complete <- dataset[rows,]
# dataset <- dataset.complete
# dim(dataset)
```

We need to sanitize the names of the categories.
```{r}
# dataset$q42 <- make.names(dataset$q42)
```

We divide the dataset into train and test.
```{r}
set.seed(1)

trainIndex <- createDataPartition(dataset$q42, p = .8, list = FALSE)
training <- dataset[ trainIndex,]
testing  <- dataset[-trainIndex,]
```

We split the data frame into numerics and factors.
```{r}
pos <- which(sapply(dataset, is.numeric))

training.fac <- select(training, -pos)
training.num <- select(training,  pos)
testing.fac <- select(testing, -pos)
testing.num <- select(testing,  pos)
```

We now fill the missing numerical values.
```{r}
preProcValues <- preProcess(training.num, method = c("center", "scale", "knnImpute", "BoxCox", "YeoJohnson"))
training.num <- predict(preProcValues, training.num)
testing.num <- predict(preProcValues, testing.num)
```

We find correlations between numerical columns.
```{r}
descrCor <- cor(training.num)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)

# Remove correlated columns if found any
if (length(highlyCorDescr) > 0) {
  training.num <- training.num[, -highlyCorDescr];
  testing.num <- testing.num[, -highlyCorDescr]
}
```

We now remove numerical columns that are linearly dependent.
```{r}
comboInfo <- findLinearCombos(training.num)
comboInfo

if (length(comboInfo$remove) > 0) {
  training.num <- training.num[, -comboInfo$remove];
  testing.num <- testing.num[, -comboInfo$remove]
}
```

We combine the numeric and factor columns back together.
```{r}
training <- cbind(training.num, training.fac)
testing <- cbind(testing.num, testing.fac)
```

We remove the rows that have too many NAs.
```{r}
# thresold.na <- 50
# training <- training[rowSums(is.na(training)) < thresold.na,]
```

We verify how many observations per predictors we have.
```{r}
nrow(training)/ncol(training)
```

We predict using regression trees.
```{r}
rtGrid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
ctrl <- trainControl(method = "cv", number = 10, verboseIter = T, classProbs = TRUE)

rtTune <- train(q42 ~ ., data = training, method = "rpart", tuneGrid = rtGrid, trControl = ctrl)

rtTune
plot(rtTune)

rtTune$bestTune
plot(rtTune$results)

par(mfrow = c(1,1))
fancyRpartPlot(rtTune$finalModel)

rtPredict <- predict(rtTune, newdata = testing, na.action = na.pass)

par(mfrow = c(1,1))
plot(rtPredict)

table(predict = rtPredict, true = testing$q42)

rtCM <- confusionMatrix(rtPredict, testing$q42, positive = "S")
rtCM
```

# Preliminary Model

We firstselect only a few columns that we think are of interest.
```{r}
dataset <- dataset.original

keep <- c("q42", "q43", "q1", "q2", "q3", "q4", "q26", "q39", "q40", "q41", "q44")
dataset.keep <- dataset[,keep]
dataset <- dataset.keep
```

We drop the rows that do not have complete information.
```{r}
rows <- complete.cases(dataset)
dataset.complete <- dataset[rows,]
dataset <- dataset.complete
```

We need to sanitize the names of the categories.
```{r}
dataset$q42 <- make.names(dataset$q42)
dataset$q43 <- make.names(dataset$q43)
```

We divide the dataset into train and test.
```{r}
set.seed(1)

trainIndex <- createDataPartition(dataset$q42, p = .8, list = FALSE)
training <- dataset[ trainIndex,]
testing  <- dataset[-trainIndex,]
```

We try to predict using regression trees.
```{r}
rtGrid <- expand.grid(cp=seq(0.01, 0.2, by = 0.005))
ctrl <- trainControl(method = "cv", number = 10, verboseIter = T, classProbs = TRUE)

rtTune <- train(q42 ~ ., data = training,
                  method = "rpart",
                  tuneGrid = rtGrid,
                  trControl = ctrl)

rtTune
plot(rtTune)

rtTune$bestTune
plot(rtTune$results)

rtPredict <- predict(rtTune, newdata = testing)
par(mfrow = c(1,1))
plot(rtPredict)

table(predict = rtPredict, true = testing$q42)

rtCM <- confusionMatrix(rtPredict, testing$q42)
rtCM

par(mfrow = c(1,1))
fancyRpartPlot(rtTune$finalModel)
```

```{r}
rtPredict <- predict(rtTune, newdata = testing, type="prob")
rtPredict$no < 0.7 # for example
rtPredict <- rtPredict %>%
  mutate(class = ifelse(yes > 0.7, "Yes", "No")) %>%
  mutate(class = as.factor(class))

table(predict = rtPredict, true = testing$q42)

rtCM <- confusionMatrix(rtPredict$class, testing$q42, positive = "Yes")
rtCM
```